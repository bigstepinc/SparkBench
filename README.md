# SparkBench [![Build Status](https://travis-ci.org/bigstepinc/SparkBench.svg?branch=master)](https://travis-ci.org/bigstepinc/SparkBench)
Terasort-like benchmark for spark 2.x that uses dataframes, parquet etc for a more realistic load testing. It is both IO, network and CPU intensive. It also puts pressure on the temporary directories and other configuration settings.

You can download the binaries from the [Releases](https://github.com/bigstepinc/SparkBench/releases).

Usage: 
```
spark-submit sparkbench.jar generate <number_of_rows> <output_directory>
            Generates a dataframe with the specified number of rows where each row has a value field with 100 characters.
spark-submit sparkbench.jar sort <output_directory> <output_directory_sorted>
            Sorts the dataframe generated by 'generate' from <output_directory> and saves the output in the <output_directory_sorted> directory.
```

- One million rows (1000000) is approximately 100MB of data.
- Ten million rows (10000000) is approximately 1GB of data. 
- Ten billion rows (10000000000) is approximately 1TB of data. 

To generate test data execute: 1m rows (100MB dataset):
```
~/spark-2.3.0-bin-hadoop2.7/bin/spark-submit sparkbench_2.11-1.0.jar generate 1000000 /input
```
To sort the test data:
```
~/spark-2.3.0-bin-hadoop2.7/bin/spark-submit sparkbench_2.11-1.0.jar sort /input /output
```

Note: the current implementation forces a repartition with the numebr of partitions equal to the number of workers available in the cluster.

##Using with S3:
Setup the conf/core-site.xml for S3:
```
<configuration>
<property>
  <name>fs.s3a.impl</name>
  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
</property>
<property>
  <name>fs.s3a.access.key</name>
  <value>ACCESSKEY</value>
</property>
<property>
  <name>fs.s3a.secret.key</name>
  <value>SECRETKEY</value>
</property>
<property>
  <name>fs.s3a.endpoint</name>
  <value>https://object.ecstestdrive.com</value>
</property>
<property>
  <name>fs.s3a.connection.ssl.enabled</name>
  <value>enabled</value>
</property>
<property>
  <name>fs.s3a.signing-algorithm</name>
  <value>S3SignerType</value>
</property>
</configuration>
```
... and generate and sort the data:
```
spark-submit --packages org.apache.hadoop:hadoop-aws:2.7.3,org.apache.hadoop:hadoop-common:2.7.3 sparkbench_2.11-1.0.jar generate 1000000 s3a://sparkgen-1m
spark-submit --packages org.apache.hadoop:hadoop-aws:2.7.3,org.apache.hadoop:hadoop-common:2.7.3 sparkbench_2.11-1.0.jar sort s3a://sparkgen-1m s3a://sparkgen-1m-sorted
```

##Using with Google GCS:
Setup the conf/core-site.xml in spark

```
<property>
  <name>google.cloud.auth.service.account.enable</name>
  <value>true</value>
</property>
<property>
  <name>google.cloud.auth.service.account.json.keyfile</name>
  <value>JSON_KEY_FILE</value>
</property>
<property>
  <name>fs.gs.impl</name>
  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
</property>
<property>
  <name>fs.AbstractFileSystem.gs.impl</name>
  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>
</property>
<property>
  <name>fs.gs.project.id</name>
  <value>GCP_PROJECT_ID</value>
</property>
```
Generate and sort data:
```
spark-submit --packages com.google.cloud.bigdataoss:gcs-connector:1.8.1-hadoop2 sparkbench_2.11-1.0.jar generate 1000000 gs://sparkgen-1m
spark-submit --packages com.google.cloud.bigdataoss:gcs-connector:1.8.1-hadoop2 sort gs://sparkgen-1m gs://sparkgen-1m-sorted
```


## Using programmatically
To use in your project as a library use something like:

```
libraryDependencies += "com.bigstep" %% "sparkbench_2.11" % "1.0" 
```
or
```
spark-submit --packages com.bigstep:sparkbench_2.11:1.0
```
... and use in your program (Scala/Java):
```
//generate values and save them
SyntheticBenchmark.create().generateRecords(1000).save("/input")
//sort values and save them.
SyntheticBenchmark.load("/input").sortByValue().save("/output")
```
